---
git_download_url: https://github.com/secretflow/spu/raw/207435da0de58cf3352ad4c7c73d97f9ef3ba9ba/docs/development/compiler.rst
git_last_modified_commit: b0fe367bd75ec9fe770245e9a45fe002665b79b9
git_last_modified_time: '2023-03-07T10:23:08+08:00'
git_origin_url: https://github.com/secretflow/spu/blob/207435da0de58cf3352ad4c7c73d97f9ef3ba9ba/docs/development/compiler.rst
git_owner: secretflow
git_repo: spu
git_revision_commit: 207435da0de58cf3352ad4c7c73d97f9ef3ba9ba
git_revision_time: '2024-01-08T11:48:55+08:00'
---

:target{#spu-compiler}

# SPU Compiler

The SPU compiler aims to provide first-party compiler support from the different ML frameworks to SPU runtime.

[MLIR](https://mlir.llvm.org/) The MLIR project is a novel approach to building reusable and extensible compiler infrastructure. MLIR aims to address software fragmentation, improve compilation for heterogeneous hardware, significantly reduce the cost of building domain specific compilers, and aid in connecting existing compilers together.

[XLA](https://www.tensorflow.org/xla/architecture) Multiple Vendors use XLA as the middle layer, mapping from platform frameworks like PyTorch, JAX, and TensorFlow into XLA and then progressively lowering down to their target hardware.

[MLIR-HLO](https://github.com/tensorflow/mlir-hlo) MLIR-HLO project connects XLA into MLIR world.

Having canonical lowerings from different frontend frameworks to the MLIR ecosystem would provide much needed relief to hardware vendors to focus on their unique value rather than implementing yet another frontend for SPU. For current hardware vendors, they just need to add LLVM target support instead of implementing separate Clang/C++ frontends. MLIR-HLO is achieving similar goal.

:target{#all-the-roads-from-ml-frameworks-to-spu}

## All the roads from ML frameworks to SPU

![](../_assets/compiler.svg)
