---
git_commit: 0883c15512c9e4d3e4ec7bbe27e038fd084a2eee
git_download_url: https://github.com/secretflow/spu/raw/0883c15512c9e4d3e4ec7bbe27e038fd084a2eee/docs/development/compiler.rst
git_origin_url: https://github.com/secretflow/spu/blob/0883c15512c9e4d3e4ec7bbe27e038fd084a2eee/docs/development/compiler.rst
git_owner: secretflow
git_repo: spu
git_timestamp: '2023-12-01T19:50:19+08:00'
---

:target{#spu-compiler}

# SPU Compiler

The SPU compiler aims to provide first-party compiler support from the different ML frameworks to SPU runtime.

[MLIR](https://mlir.llvm.org/) The MLIR project is a novel approach to building reusable and extensible compiler infrastructure. MLIR aims to address software fragmentation, improve compilation for heterogeneous hardware, significantly reduce the cost of building domain specific compilers, and aid in connecting existing compilers together.

[XLA](https://www.tensorflow.org/xla/architecture) Multiple Vendors use XLA as the middle layer, mapping from platform frameworks like PyTorch, JAX, and TensorFlow into XLA and then progressively lowering down to their target hardware.

[MLIR-HLO](https://github.com/tensorflow/mlir-hlo) MLIR-HLO project connects XLA into MLIR world.

Having canonical lowerings from different frontend frameworks to the MLIR ecosystem would provide much needed relief to hardware vendors to focus on their unique value rather than implementing yet another frontend for SPU. For current hardware vendors, they just need to add LLVM target support instead of implementing separate Clang/C++ frontends. MLIR-HLO is achieving similar goal.

:target{#all-the-roads-from-ml-frameworks-to-spu}

## All the roads from ML frameworks to SPU

![](../_assets/compiler.svg)
