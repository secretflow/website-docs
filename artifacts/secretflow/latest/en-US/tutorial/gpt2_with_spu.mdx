---
git_commit: 215ee402b76decfeb01e97c0861cab9f5ec3f823
git_download_url: https://github.com/secretflow/secretflow/raw/215ee402b76decfeb01e97c0861cab9f5ec3f823/docs/tutorial/gpt2_with_spu.ipynb
git_origin_url: https://github.com/secretflow/secretflow/blob/215ee402b76decfeb01e97c0861cab9f5ec3f823/docs/tutorial/gpt2_with_spu.ipynb
git_owner: secretflow
git_repo: secretflow
git_timestamp: '2023-12-06T20:08:22+08:00'
---

:target{#GPT-2-private-inference-with-SPU}

# GPT-2 private inference with SPU

In lab [Neural Network with SPU](nn_with_spu.mdx), we have demonstrated how to use SecretFlow/SPU to train a Neural Network model privately.

In this lab, we showcase how to run private inference on a pre-trained [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) model for text generation with SPU.

First, we show how to use JAX and the Hugging Face Transformers library for text generation with the pre-trained GPT-2 model. After that, we show how to use SPU for private text generation with minor modifications to the plaintext counterpart.

> The following codes are demos only. It’s <strong>NOT for production</strong> due to system security concerns, please <strong>DO NOT</strong> use it directly in production.

> This tutorial may need more resources than 16c48g.

:target{#Text-generation-using-GPT-2-with-JAX/FLAX}

## Text generation using GPT-2 with JAX/FLAX

:target{#Install-the-transformers-library}

### Install the transformers library

<Notebook.Cell>
  <Notebook.CodeArea prompt="[ ]:" stderr={false} type="input">
    ```python
    import sys

    !{sys.executable} -m pip install transformers[flax]
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

> The JAX version required by transformers is not satisfied with SPU. But it’s ok to run with the conflicted JAX with SPU in this example.

:target{#Load-the-pre-trained-GPT-2-Model}

### Load the pre-trained GPT-2 Model

Please refer to this [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/gpt2) for more details.

<Notebook.Cell>
  <Notebook.CodeArea prompt="[2]:" stderr={false} type="input">
    ```python
    from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config

    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained("gpt2")
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Define-the-text-generation-function}

### Define the text generation function

We use a [greedy search strategy](https://huggingface.co/blog/how-to-generate) for text generation here.

<Notebook.Cell>
  <Notebook.CodeArea prompt="[3]:" stderr={false} type="input">
    ```python
    def text_generation(input_ids, params):
        config = GPT2Config()
        model = FlaxGPT2LMHeadModel(config=config)

        for _ in range(10):
            outputs = model(input_ids=input_ids, params=params)
            next_token_logits = outputs[0][0, -1, :]
            next_token = jnp.argmax(next_token_logits)
            input_ids = jnp.concatenate([input_ids, jnp.array([[next_token]])], axis=1)
        return input_ids
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Run-text-generation-on-CPU}

### Run text generation on CPU

<Notebook.Cell>
  <Notebook.CodeArea prompt="[4]:" stderr={false} type="input">
    ```python
    import jax.numpy as jnp

    inputs_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='jax')
    outputs_ids = text_generation(inputs_ids, pretrained_model.params)

    print('-' * 65 + '\nRun on CPU:\n' + '-' * 65)
    print(tokenizer.decode(outputs_ids[0], skip_special_tokens=True))
    print('-' * 65)
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="" stderr={false} type="output">
    <pre>
      {"-----------------------------------------------------------------\nRun on CPU:\n-----------------------------------------------------------------\nI enjoy walking with my cute dog, but I'm not sure if I'll ever\n-----------------------------------------------------------------\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

Here we generate 10 tokens. Keep the generated text in mind, we are going to generate text on SPU in the next step.

:target{#Run-text-generation-on-SPU}

### Run text generation on SPU

<Notebook.Cell>
  <Notebook.CodeArea prompt="[5]:" stderr={false} type="input">
    ```python
    import secretflow as sf

    # In case you have a running secretflow runtime already.
    sf.shutdown()

    sf.init(['alice', 'bob', 'carol'], address='local')

    alice, bob = sf.PYU('alice'), sf.PYU('bob')
    conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])
    conf['runtime_config']['fxp_exp_mode'] = 1
    conf['runtime_config']['experimental_disable_mmul_split'] = True
    spu = sf.SPU(conf)


    def get_model_params():
        pretrained_model = FlaxGPT2LMHeadModel.from_pretrained("gpt2")
        return pretrained_model.params


    def get_token_ids():
        tokenizer = AutoTokenizer.from_pretrained("gpt2")
        return tokenizer.encode('I enjoy walking with my cute dog', return_tensors='jax')


    model_params = alice(get_model_params)()
    input_token_ids = bob(get_token_ids)()

    device = spu
    model_params_, input_token_ids_ = model_params.to(device), input_token_ids.to(device)

    output_token_ids = spu(text_generation)(input_token_ids_, model_params_)
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="" stderr={false} type="output">
    <pre>
      {"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"}
    </pre>
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="" stderr={false} type="output">
    <pre>
      {"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"}
    </pre>
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="" stderr={false} type="output">
    <pre>
      <span className="ansi-cyan-fg">{"(_run pid=2109408)"}</span>{" [2023-06-15 17:08:24.221] [info] [thread_pool.cc:30] Create a fixed thread pool with size 127\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Check-the-SPU-output}

### Check the SPU output

As you can see, it’s very easy to run GPT-2 inference on SPU. Now let’s reveal the generated text from SPU program.

<Notebook.Cell>
  <Notebook.CodeArea prompt="[6]:" stderr={false} type="input">
    ```python
    outputs_ids = sf.reveal(output_token_ids)
    print('-' * 65 + '\nRun on SPU:\n' + '-' * 65)
    print(tokenizer.decode(outputs_ids[0], skip_special_tokens=True))
    print('-' * 65)
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="" stderr={false} type="output">
    <pre>
      <span className="ansi-cyan-fg">{"(SPURuntime(device_id=None, party=bob) pid=2121303)"}</span>{" 2023-06-15 17:09:32.011 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n"}<span className="ansi-cyan-fg">{"(SPURuntime(device_id=None, party=alice) pid=2121301)"}</span>{" 2023-06-15 17:09:32.011 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n"}<span className="ansi-cyan-fg">{"(SPURuntime(device_id=None, party=carol) pid=2121304)"}</span>{" 2023-06-15 17:09:32.011 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n-----------------------------------------------------------------\nRun on SPU:\n-----------------------------------------------------------------\nI enjoy walking with my cute dog, but I'm not sure if I'll ever\n-----------------------------------------------------------------\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

As we can see, the generated text from SPU is exactly same as the generated text from CPU!

This is the end of the lab.
