---
git_commit: 215ee402b76decfeb01e97c0861cab9f5ec3f823
git_download_url: https://github.com/secretflow/secretflow/raw/215ee402b76decfeb01e97c0861cab9f5ec3f823/docs/tutorial/Federated_Xgboost.ipynb
git_origin_url: https://github.com/secretflow/secretflow/blob/215ee402b76decfeb01e97c0861cab9f5ec3f823/docs/tutorial/Federated_Xgboost.ipynb
git_owner: secretflow
git_repo: secretflow
git_timestamp: '2023-12-06T20:08:22+08:00'
---

:target{#Horizontally-Federated-XGBoost}

# Horizontally Federated XGBoost

> The following codes are demos only. It’s <strong>NOT for production</strong> due to system security concerns, please <strong>DO NOT</strong> use it directly in production.

In this tutorial, we will learn how to use SecretFlow to train tree models for horizontal federation. Secretflow provides tree modeling capabilities for horizontal scenarios(SFXgboost), The usage of SFXgboost is similar to XGBoost, you can easily convert your existing XGBoost program into a federated model for SecretFlow.

:target{#Xgboost}

## Xgboost

XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework.

official tutorial [XGBoost tutorials](https://xgboost.readthedocs.io/en/latest/tutorials/index.html).

:target{#prepare-secretflow-devices}

### prepare secretflow devices

<Notebook.Cell>
  <Notebook.CodeArea prompt="[1]:" stderr={false} type="input">
    ```python
    %load_ext autoreload
    %autoreload 2

    import secretflow as sf

    # Check the version of your SecretFlow
    print('The version of SecretFlow: {}'.format(sf.__version__))

    # In case you have a running secretflow runtime already.
    sf.shutdown()

    sf.init(['alice', 'bob', 'charlie'], address='local')
    alice, bob, charlie = sf.PYU('alice'), sf.PYU('bob'), sf.PYU('charlie')
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#XGBoost-Example}

### XGBoost Example

<Notebook.Cell>
  <Notebook.CodeArea prompt="[2]:" stderr={false} type="input">
    ```python
    import xgboost as xgb
    import pandas as pd
    from secretflow.utils.simulation.datasets import dataset

    df = pd.read_csv(dataset('dermatology'))
    df.fillna(value=0)
    print(df.dtypes)
    y = df['class']
    y = y - 1
    x = df.drop(columns="class")
    dtrain = xgb.DMatrix(x, y)
    dtest = dtrain
    params = {
        'max_depth': 4,
        'objective': 'multi:softmax',
        'min_child_weight': 1,
        'max_bin': 10,
        'num_class': 6,
        'eval_metric': 'merror',
    }
    num_round = 4
    watchlist = [(dtrain, 'train')]
    bst = xgb.train(params, dtrain, num_round, evals=watchlist, early_stopping_rounds=2)
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="" stderr={false} type="output">
    <pre>
      {"erythema                                      int64\nscaling                                       int64\ndefinite_borders                              int64\nitching                                       int64\nkoebner_phenomenon                            int64\npolygonal_papules                             int64\nfollicular_papules                            int64\noral_mucosal_involvement                      int64\nknee_and_elbow_involvement                    int64\nscalp_involvement                             int64\nfamily_history                                int64\nmelanin_incontinence                          int64\neosinophils_in_the_infiltrate                 int64\npnl_infiltrate                                int64\nfibrosis_of_the_papillary_dermis              int64\nexocytosis                                    int64\nacanthosis                                    int64\nhyperkeratosis                                int64\nparakeratosis                                 int64\nclubbing_of_the_rete_ridges                   int64\nelongation_of_the_rete_ridges                 int64\nthinning_of_the_suprapapillary_epidermis      int64\nspongiform_pustule                            int64\nmunro_microabcess                             int64\nfocal_hypergranulosis                         int64\ndisappearance_of_the_granular_layer           int64\nvacuolisation_and_damage_of_basal_layer       int64\nspongiosis                                    int64\nsaw-tooth_appearance_of_retes                 int64\nfollicular_horn_plug                          int64\nperifollicular_parakeratosis                  int64\ninflammatory_monoluclear_inflitrate           int64\nband-like_infiltrate                          int64\nage                                         float64\nclass                                         int64\ndtype: object\n[0]     train-merror:0.01913\n[1]     train-merror:0.01366\n[2]     train-merror:0.01366\n[3]     train-merror:0.00820\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Then,-How-to-do-federated-xgboost-in-secretflow?}

### Then, How to do federated xgboost in secretflow?

1. Use federate Binning method based on iteration to calculate the global bucket information combined with the data of all sides, which was used as the candidate to enter the subsequent construction procedure.
2. The data is input into each Client XGBoost engine to calculate G & H.
3. Train federated boosting model
   1. Data is reassigned to the node to be split.
   2. The sum of grad and the sum of hess are calculated according to the previously calculated binning buckets.
   3. Send the sum of grad and the sum of hess to server，server use secure aggregation to produce global summary，then choose best split point，Send best split info back to clients.
   4. Clients Updates local model.
4. Finish training，and save model.

Create 3 entities in the Secretflow environment \[Alice, Bob, Charlie]. Where `Alice` and `Bob` are clients, and `Charlie` is the server,then you can happily start `Federate Boosting`.

:target{#Prepare-Data}

### Prepare Data

<Notebook.Cell>
  <Notebook.CodeArea prompt="[3]:" stderr={false} type="input">
    ```python
    from secretflow.data.horizontal import read_csv
    from secretflow.security.aggregation import SecureAggregator
    from secretflow.security.compare import SPUComparator
    from secretflow.utils.simulation.datasets import load_dermatology

    aggr = SecureAggregator(charlie, [alice, bob])
    spu = sf.SPU(sf.utils.testing.cluster_def(['alice', 'bob']))
    comp = SPUComparator(spu)
    data = load_dermatology(parts=[alice, bob], aggregator=aggr, comparator=comp)
    data.fillna(value=0, inplace=True)
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Prepare-Params}

### Prepare Params

<Notebook.Cell>
  <Notebook.CodeArea prompt="[4]:" stderr={false} type="input">
    ```python
    params = {
        # XGBoost parameter tutorial
        # https://xgboost.readthedocs.io/en/latest/parameter.html
        'max_depth': 4,  # max depth
        'eta': 0.3,  # learning rate
        'objective': 'multi:softmax',  # objection function，support "binary:logistic","reg:logistic","multi:softmax","multi:softprob","reg:squarederror"
        'min_child_weight': 1,  # The minimum value of weight
        'lambda': 0.1,  # L2 regularization term on weights (xgb's lambda)
        'alpha': 0,  # L1 regularization term on weights (xgb's alpha)
        'max_bin': 10,  # Max num of binning
        'num_class': 6,  # Only required in multi-class classification
        'gamma': 0,  # Same to min_impurity_split,The minimux gain for a split
        'subsample': 1.0,  # Subsample rate by rows
        'colsample_bytree': 1.0,  # Feature selection rate by tree
        'colsample_bylevel': 1.0,  # Feature selection rate by level
        'eval_metric': 'merror',  # supported eval metric：
        # 1. rmse
        # 2. rmsle
        # 3. mape
        # 4. logloss
        # 5. error
        # 6. error@t
        # 7. merror
        # 8. mlogloss
        # 9. auc
        # 10. aucpr
        # Special params in SFXgboost
        # Required
        'hess_key': 'hess',  # Required, Mark hess columns, optionally choosing a column name that is not in the data set
        'grad_key': 'grad',  # Required，Mark grad columns, optionally choosing a column name that is not in the data set
        'label_key': 'class',  # Required，ark label columns, optionally choosing a column name that is not in the data set
    }
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Create-SFXgboost}

### Create SFXgboost

<Notebook.Cell>
  <Notebook.CodeArea prompt="[6]:" stderr={false} type="input">
    ```python
    from secretflow.ml.boost.homo_boost import SFXgboost

    bst = SFXgboost(server=charlie, clients=[alice, bob])
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

run SFXgboost

<Notebook.Cell>
  <Notebook.CodeArea prompt="[7]:" stderr={false} type="input">
    ```python
    bst.train(data, data, params=params, num_boost_round=6)
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="" stderr={false} type="output">
    <pre>
      <span className="ansi-cyan-fg">{"(HomoBooster pid=3817964)"}</span>{" [0] train-merror:0.01366    valid-merror:0.01366\n"}<span className="ansi-cyan-fg">{"(HomoBooster pid=3817954)"}</span>{" [0] train-merror:0.01366    valid-merror:0.01366\n"}<span className="ansi-cyan-fg">{"(HomoBooster pid=3817963)"}</span>{" [0] train-merror:0.01366    valid-merror:0.01366\n"}<span className="ansi-cyan-fg">{"(HomoBooster pid=3817964)"}</span>{" [1] train-merror:0.00820    valid-merror:0.00820\n"}<span className="ansi-cyan-fg">{"(HomoBooster pid=3817954)"}</span>{" [1] train-merror:0.00820    valid-merror:0.00820\n"}<span className="ansi-cyan-fg">{"(HomoBooster pid=3817963)"}</span>{" [1] train-merror:0.00820    valid-merror:0.00820\n"}<span className="ansi-cyan-fg">{"(HomoBooster pid=3817964)"}</span>{" [2] train-merror:0.00820    valid-merror:0.00820\n"}<span className="ansi-cyan-fg">{"(HomoBooster pid=3817954)"}</span>{" [2] train-merror:0.00820    valid-merror:0.00820\n"}<span className="ansi-cyan-fg">{"(HomoBooster pid=3817963)"}</span>{" [2] train-merror:0.00820    valid-merror:0.00820\n"}<span className="ansi-cyan-fg">{"(HomoBooster pid=3817964)"}</span>{" [3] train-merror:0.01093    valid-merror:0.01093\n"}<span className="ansi-cyan-fg">{"(HomoBooster pid=3817954)"}</span>{" [3] train-merror:0.01093    valid-merror:0.01093\n"}<span className="ansi-cyan-fg">{"(HomoBooster pid=3817963)"}</span>{" [3] train-merror:0.01093    valid-merror:0.01093\n"}<span className="ansi-cyan-fg">{"(HomoBooster pid=3817964)"}</span>{" [4] train-merror:0.00820    valid-merror:0.00820\n"}<span className="ansi-cyan-fg">{"(HomoBooster pid=3817954)"}</span>{" [4] train-merror:0.00820    valid-merror:0.00820\n"}<span className="ansi-cyan-fg">{"(HomoBooster pid=3817963)"}</span>{" [4] train-merror:0.00820    valid-merror:0.00820\n"}<span className="ansi-cyan-fg">{"(HomoBooster pid=3817964)"}</span>{" [5] train-merror:0.00546    valid-merror:0.00546\n"}<span className="ansi-cyan-fg">{"(HomoBooster pid=3817954)"}</span>{" [5] train-merror:0.00546    valid-merror:0.00546\n"}<span className="ansi-cyan-fg">{"(HomoBooster pid=3817963)"}</span>{" [5] train-merror:0.00546    valid-merror:0.00546\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

Now our Federated XGBoost training is complete, where the BST is the federated Boost object.

:target{#Conclusion}

## Conclusion

- This tutorial introduces how to use tree models for training etc.
- SFXgboost encapsulates the logic of the federated subtree model. Sfxgboost trained models remain compatible with XGBoost, and we can directly use the existing infrastructure for online prediction and so on.
- Next, you can try SFXgboost on your data, just need to follow this tutorial.
