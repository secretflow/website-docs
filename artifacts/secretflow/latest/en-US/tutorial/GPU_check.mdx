---
git_download_url: https://github.com/secretflow/secretflow/raw/215ee402b76decfeb01e97c0861cab9f5ec3f823/docs/tutorial/GPU_check.ipynb
git_last_modified_commit: a06f2ac1994afc5af895824e2000a848dd1e4673
git_last_modified_time: '2023-08-22T13:04:56+08:00'
git_origin_url: https://github.com/secretflow/secretflow/blob/215ee402b76decfeb01e97c0861cab9f5ec3f823/docs/tutorial/GPU_check.ipynb
git_owner: secretflow
git_repo: secretflow
git_revision_commit: 215ee402b76decfeb01e97c0861cab9f5ec3f823
git_revision_time: '2023-12-06T20:08:22+08:00'
---

:target{#GPU-check}

# GPU check

:target{#Check-NVIDIA-driver}

## Check NVIDIA driver

<Notebook.Cell>
  <Notebook.CodeArea prompt="[1]:" stderr={false} type="input">
    ```python
    !nvidia-smi
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="" stderr={false} type="output">
    <pre>
      {"Thu May 18 03:47:56 2023\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:07.0 Off |                    0 |\n| N/A   45C    P0    27W /  70W |      2MiB / 15360MiB |      6%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Check-GPU-in-PyTorch}

## Check GPU in PyTorch

:target{#Import-PyTorch-to-check-GPU-devices}

### Import PyTorch to check GPU devices

<Notebook.Cell>
  <Notebook.CodeArea prompt="[2]:" stderr={false} type="input">
    ```python
    import torch
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Check-the-version-of-PyTorch}

### Check the version of PyTorch

<Notebook.Cell>
  <Notebook.CodeArea prompt="[3]:" stderr={false} type="input">
    ```python
    torch.__version__
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="[3]:" stderr={false} type="output">
    <pre>
      {"'2.0.0+cu117'\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Check-if-PyTorch-can-call-GPUs}

### Check if PyTorch can call GPUs

<Notebook.Cell>
  <Notebook.CodeArea prompt="[4]:" stderr={false} type="input">
    ```python
    torch.cuda.is_available()
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="[4]:" stderr={false} type="output">
    <pre>
      {"True\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Check-the-number-of-the-GPUs-of-the-computer-in-PyTorch}

### Check the number of the GPUs of the computer in PyTorch

<Notebook.Cell>
  <Notebook.CodeArea prompt="[5]:" stderr={false} type="input">
    ```python
    gpu_num = torch.cuda.device_count()
    print(gpu_num)
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="" stderr={false} type="output">
    <pre>
      {"1\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Check-the-GPU-type-of-the-computer-in-PyTorch}

### Check the GPU type of the computer in PyTorch

<Notebook.Cell>
  <Notebook.CodeArea prompt="[6]:" stderr={false} type="input">
    ```python
    for i in range(gpu_num):
        print('GPU {}.: {}'.format(i, torch.cuda.get_device_name(i)))
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="" stderr={false} type="output">
    <pre>
      {"GPU 0.: Tesla T4\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Check-GPU-in-TensorFlow}

## Check GPU in TensorFlow

:target{#Import-TensorFlow-to-check-GPU-devices}

### Import TensorFlow to check GPU devices

<Notebook.Cell>
  <Notebook.CodeArea prompt="[7]:" stderr={false} type="input">
    ```python
    import tensorflow as tf
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Check-the-version-of-TensorFlow}

### Check the version of TensorFlow

<Notebook.Cell>
  <Notebook.CodeArea prompt="[8]:" stderr={false} type="input">
    ```python
    tf.__version__
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="[8]:" stderr={false} type="output">
    <pre>
      {"'2.12.0'\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Check-if-TensorFlow-can-call-GPUs}

### Check if TensorFlow can call GPUs

<Notebook.Cell>
  <Notebook.CodeArea prompt="[9]:" stderr={false} type="input">
    ```python
    tf.test.is_gpu_available()
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="" stderr={false} type="output">
    <pre>
      {"WARNING:tensorflow:From /tmp/ipykernel_3029/337460670.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.config.list_physical_devices('GPU')` instead.\n"}
    </pre>
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="[9]:" stderr={false} type="output">
    <pre>
      {"True\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Check-physical-GPUs-in-TensorFlow}

### Check physical GPUs in TensorFlow

<Notebook.Cell>
  <Notebook.CodeArea prompt="[10]:" stderr={false} type="input">
    ```python
    tf.config.list_physical_devices('GPU')
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="[10]:" stderr={false} type="output">
    <pre>
      {"[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Check-the-GPU-type-of-the-computer-in-TensorFlow}

### Check the GPU type of the computer in TensorFlow

<Notebook.Cell>
  <Notebook.CodeArea prompt="[11]:" stderr={false} type="input">
    ```python
    from tensorflow.python.client import device_lib

    local_device_protos = device_lib.list_local_devices()
    for x in local_device_protos:
        if x.device_type == 'GPU':
            print(x.physical_device_desc)
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="" stderr={false} type="output">
    <pre>
      {"device: 0, name: Tesla T4, pci bus id: 0000:00:07.0, compute capability: 7.5\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Check-GPU-in-Jax-and-jaxlib}

## Check GPU in Jax and jaxlib

:target{#Import-Jax-and-jaxlib-to-check-GPU-devices}

### Import Jax and jaxlib to check GPU devices

<Notebook.Cell>
  <Notebook.CodeArea prompt="[12]:" stderr={false} type="input">
    ```python
    import jax
    import jaxlib
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Check-the-version-of-jax-and-jaxlib}

### Check the version of jax and jaxlib

<Notebook.Cell>
  <Notebook.CodeArea prompt="[13]:" stderr={false} type="input">
    ```python
    jax.__version__
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="[13]:" stderr={false} type="output">
    <pre>
      {"'0.4.1'\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

<Notebook.Cell>
  <Notebook.CodeArea prompt="[14]:" stderr={false} type="input">
    ```python
    jaxlib.__version__
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="[14]:" stderr={false} type="output">
    <pre>
      {"'0.4.1'\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Check-all-the-devices-from-the-default-backend-of-jax}

### Check all the devices from the default backend of jax

<Notebook.Cell>
  <Notebook.CodeArea prompt="[15]:" stderr={false} type="input">
    ```python
    jax.devices()
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="[15]:" stderr={false} type="output">
    <pre>
      {"[StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)]\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Check-the-total-number-of-devices-of-jax}

### Check the total number of devices of jax

<Notebook.Cell>
  <Notebook.CodeArea prompt="[16]:" stderr={false} type="input">
    ```python
    jax.device_count()
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="[16]:" stderr={false} type="output">
    <pre>
      {"1\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Check-the-number-of-JAX-processes-associated-with-the-backend-of-jax}

### Check the number of JAX processes associated with the backend of jax

<Notebook.Cell>
  <Notebook.CodeArea prompt="[17]:" stderr={false} type="input">
    ```python
    jax.process_count()
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="[17]:" stderr={false} type="output">
    <pre>
      {"1\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Check-the-backend-of-jaxlib}

### Check the backend of jaxlib

<Notebook.Cell>
  <Notebook.CodeArea prompt="[18]:" stderr={false} type="input">
    ```python
    from jax.lib import xla_bridge

    print(xla_bridge.get_backend().platform)
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="" stderr={false} type="output">
    <pre>
      {"gpu\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Run-a-linear-regression-demo-to-verify-that-the-Jax-can-run-well}

### Run a linear regression demo to verify that the Jax can run well

The demo code is from [https://www.secretflow.org.cn/docs/secretflow/en/tutorial/lr\_with\_spu.html](https://www.secretflow.org.cn/docs/secretflow/en/tutorial/lr_with_spu.html) #### load dataset

<Notebook.Cell>
  <Notebook.CodeArea prompt="[19]:" stderr={false} type="input">
    ```python
    import numpy as np
    from sklearn.datasets import load_breast_cancer
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import Normalizer


    def breast_cancer(party_id=None, train: bool = True) -> (np.ndarray, np.ndarray):
        x, y = load_breast_cancer(return_X_y=True)
        x = (x - np.min(x)) / (np.max(x) - np.min(x))
        x_train, x_test, y_train, y_test = train_test_split(
            x, y, test_size=0.2, random_state=42
        )

        if train:
            if party_id:
                if party_id == 1:
                    return x_train[:, :15], _
                else:
                    return x_train[:, 15:], y_train
            else:
                return x_train, y_train
        else:
            return x_test, y_test
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#define-model}

#### define model

<Notebook.Cell>
  <Notebook.CodeArea prompt="[20]:" stderr={false} type="input">
    ```python
    import jax.numpy as jnp


    def sigmoid(x):
        return 1 / (1 + jnp.exp(-x))


    # Outputs probability of a label being true.
    def predict(W, b, inputs):
        return sigmoid(jnp.dot(inputs, W) + b)


    # Training loss is the negative log-likelihood of the training examples.
    def loss(W, b, inputs, targets):
        preds = predict(W, b, inputs)
        label_probs = preds * targets + (1 - preds) * (1 - targets)
        return -jnp.mean(jnp.log(label_probs))
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#build-train-step}

#### build train step

<Notebook.Cell>
  <Notebook.CodeArea prompt="[21]:" stderr={false} type="input">
    ```python
    from jax import grad


    def train_step(W, b, x1, x2, y, learning_rate):
        x = jnp.concatenate([x1, x2], axis=1)
        Wb_grad = grad(loss, (0, 1))(W, b, x, y)
        W -= learning_rate * Wb_grad[0]
        b -= learning_rate * Wb_grad[1]
        return W, b
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#build-fit-function}

#### build fit function

<Notebook.Cell>
  <Notebook.CodeArea prompt="[22]:" stderr={false} type="input">
    ```python
    def fit(W, b, x1, x2, y, epochs=1, learning_rate=1e-2):
        for _ in range(epochs):
            W, b = train_step(W, b, x1, x2, y, learning_rate=learning_rate)
        return W, b
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#validate-model}

#### validate model

<Notebook.Cell>
  <Notebook.CodeArea prompt="[23]:" stderr={false} type="input">
    ```python
    from sklearn.metrics import roc_auc_score


    def validate_model(W, b, X_test, y_test):
        y_pred = predict(W, b, X_test)
        return roc_auc_score(y_test, y_pred)
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#train-model}

#### train model

<Notebook.Cell>
  <Notebook.CodeArea prompt="[24]:" stderr={false} type="input">
    ```python
    %matplotlib inline

    # Load the data
    x1, _ = breast_cancer(party_id=1, train=True)
    x2, y = breast_cancer(party_id=2, train=True)

    # Hyperparameter
    W = jnp.zeros((30,))
    b = 0.0
    epochs = 10
    learning_rate = 1e-2

    # Train the model
    W, b = fit(W, b, x1, x2, y, epochs=10, learning_rate=1e-2)

    # Validate the model
    X_test, y_test = breast_cancer(train=False)
    auc = validate_model(W, b, X_test, y_test)
    print(f'auc={auc}')
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="" stderr={false} type="output">
    <pre>
      {"auc=0.9880445463478545\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

As you can see, the warning message

> No GPU/TPU found, falling back to CPU. (Set TF\_CPP\_MIN\_LOG\_LEVEL=0 and rerun for more info.)

doesnâ€™t appear, which indicate the jax run the code in GPU

:target{#Import-SecretFlow-to-verify-that-no-errors-are-reported-in-this-environemnt}

## Import SecretFlow to verify that no errors are reported in this environemnt

<Notebook.Cell>
  <Notebook.CodeArea prompt="[25]:" stderr={false} type="input">
    ```python
    import secretflow
    ```
  </Notebook.CodeArea>
</Notebook.Cell>
