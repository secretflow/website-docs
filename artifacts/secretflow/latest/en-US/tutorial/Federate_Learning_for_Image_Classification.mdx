:target{#Federated-Learning-for-Image-Classification}

# Federated Learning for Image Classification

> The following codes are demos only. It’s <strong>NOT for production</strong> due to system security concerns, please <strong>DO NOT</strong> use it directly in production.

In this tutorial, we will use the image classification task to show how to complete the horizontal federated learning task in the `SecretFlow` framework. The `SecretFlow` framework provides a user-friendly API that makes it easy to apply your Keras or PyTorch model to a federated learning scenario as a federated learning model. In the rest of the tutorial we will show you how to turn your existing model into a federated model in `SecretFlow` to complete federated multi-party modeling
tasks.

:target{#What-is-Federated-Learning}

## What is Federated Learning

The federated learning discussed here is specifically focused on horizontal scenarios, where each participant shares the same business but reaches different customer groups. This allows samples from different parties to be combined to train a joint model with improved performance. An example of this scenario can be found in the medical field, where each hospital has its own distinctive patient group and hospitals in different regions largely do not overlap. However, their medical records (such
as images and blood tests) for diagnostic purposes are of the same type.

![federate\_learning.png](../_assets/federate_learning.png)

Training process:

1. Each participant downloads the latest model from the server.
2. Each participant uses its own local data to train the model, and uploads gradient encryption (or parameter encryption) to the server, which obtains the encryption gradient (encryption parameter) uploaded by all parties for security aggregation at the server, and updates model parameters with the aggregated gradient.
3. The server returns the updated model to each participant.
4. Each participant updates their local model, and prepare next training.

:target{#Federated-learning-on-SecretFlow}

## Federated learning on SecretFlow

<Notebook.Cell>
  <Notebook.CodeArea prompt="[1]:" stderr={false} type="input">
    ```python
    %load_ext autoreload
    %autoreload 2
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

Create 3 entities in the Secretflow environment \[Alice, Bob, Charlie]. Alice, Bob and Charlie are the three PYUs. Alice and Bob to be the clients and Charlie to be the server.

<Notebook.Cell>
  <Notebook.CodeArea prompt="[2]:" stderr={false} type="input">
    ```python
    import secretflow as sf

    # Check the version of your SecretFlow
    print('The version of SecretFlow: {}'.format(sf.__version__))

    # In case you have a running secretflow runtime already.
    sf.shutdown()

    sf.init(['alice', 'bob', 'charlie'], address='local')
    alice, bob, charlie = sf.PYU('alice'), sf.PYU('bob'), sf.PYU('charlie')
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

<Notebook.Cell>
  <Notebook.CodeArea prompt="[3]:" stderr={false} type="input">
    ```python
    spu = sf.SPU(sf.utils.testing.cluster_def(['alice', 'bob']))
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Prepare-Data}

### Prepare Data

Alice and Bob each own half the data.

<Notebook.Cell>
  <Notebook.CodeArea prompt="[4]:" stderr={false} type="input">
    ```python
    from secretflow.data.ndarray import load
    from secretflow.utils.simulation.datasets import load_mnist

    (x_train, y_train), (x_test, y_test) = load_mnist(
        parts=[alice, bob], normalized_x=True, categorical_y=True
    )
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

`x_train`, `y_train`, `x_test`, `y_test` are both `FedNdarray`. Let’s take a look at the data obtained from FedNdarray. FedNdarray is a virtual Ndarray built on a multi-party concept to protect data privacy. The underlying data is stored in each participant. The FedNdarray operation is actually performed by each participant on their own local data. The server or other clients do not touch the original data. For demonstration purposes, we will manually download the data to the driver.
<strong>This data will be used later in the unilateral model comparison</strong>.

<Notebook.Cell>
  <Notebook.CodeArea prompt="[5]:" stderr={false} type="input">
    ```python
    import numpy as np
    from secretflow.utils.simulation.datasets import dataset

    mnist = np.load(dataset('mnist'), allow_pickle=True)
    image = mnist['x_train']
    label = mnist['y_train']
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

Let’s grab some samples from the data set, and just visually see, what does the data look like for Both Alice and Bob?

<Notebook.Cell>
  <Notebook.CodeArea prompt="[6]:" stderr={false} type="input">
    ```python
    from matplotlib import pyplot as plt

    figure = plt.figure(figsize=(20, 4))
    j = 0

    for example in image[:40]:
        plt.subplot(4, 10, j + 1)
        plt.imshow(example, cmap='gray', aspect='equal')
        plt.axis('off')
        j += 1
    ```
  </Notebook.CodeArea>

  <Notebook.FancyOutput prompt="" type="output">
    ![](../_assets/tutorial_Federate_Learning_for_Image_Classification_17_0.png)
  </Notebook.FancyOutput>
</Notebook.Cell>

<Notebook.Cell>
  <Notebook.CodeArea prompt="[7]:" stderr={false} type="input">
    ```python
    figure = plt.figure(figsize=(20, 4))
    j = 0
    for example in image[:40]:
        plt.subplot(4, 10, j + 1)
        plt.imshow(example, cmap='gray', aspect='equal')
        plt.axis('off')
        j += 1
    ```
  </Notebook.CodeArea>

  <Notebook.FancyOutput prompt="" type="output">
    ![](../_assets/tutorial_Federate_Learning_for_Image_Classification_18_0.png)
  </Notebook.FancyOutput>
</Notebook.Cell>

It can be seen from the above two examples that the data types and tasks of Alice and Bob are consistent, but the samples are different due to the different user groups they reach.

:target{#Define-Model}

### Define Model

<Notebook.Cell>
  <Notebook.CodeArea prompt="[8]:" stderr={false} type="input">
    ```python
    def create_conv_model(input_shape, num_classes, name='model'):
        def create_model():
            from tensorflow import keras
            from tensorflow.keras import layers

            # Create model
            model = keras.Sequential(
                [
                    keras.Input(shape=input_shape),
                    layers.Conv2D(32, kernel_size=(3, 3), activation="relu"),
                    layers.MaxPooling2D(pool_size=(2, 2)),
                    layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
                    layers.MaxPooling2D(pool_size=(2, 2)),
                    layers.Flatten(),
                    layers.Dropout(0.5),
                    layers.Dense(num_classes, activation="softmax"),
                ]
            )
            # Compile model
            model.compile(
                loss='categorical_crossentropy', optimizer='adam', metrics=["accuracy"]
            )
            return model

        return create_model
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Training-FL-Model}

### Training FL Model

1. Import packages

<Notebook.Cell>
  <Notebook.CodeArea prompt="[9]:" stderr={false} type="input">
    ```python
    from secretflow.security.aggregation import SPUAggregator, SecureAggregator
    from secretflow.ml.nn import FLModel
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

2. Define Model

<Notebook.Cell>
  <Notebook.CodeArea prompt="[10]:" stderr={false} type="input">
    ```python
    num_classes = 10
    input_shape = (28, 28, 1)
    model = create_conv_model(input_shape, num_classes)
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

3. Define the device list for participating training, which is the PYUS of each participant prepared previously.

<Notebook.Cell>
  <Notebook.CodeArea prompt="[11]:" stderr={false} type="input">
    ```python
    device_list = [alice, bob]
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

4. Define Aggregator
   The SecretFlow framework provides a variety of aggregation schemes, including `SecureAggregator` and `PPUAggregator`, which can be used for secure aggregation. To learn more information about aggregation, see [Secure Aggregator](../developer/algorithm/secure_aggregation.mdx).

<Notebook.Cell>
  <Notebook.CodeArea prompt="[12]:" stderr={false} type="input">
    ```python
    secure_aggregator = SecureAggregator(charlie, [alice, bob])
    spu_aggregator = SPUAggregator(spu)
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

5. Define FLModel

<Notebook.Cell>
  <Notebook.CodeArea prompt="[13]:" stderr={false} type="input">
    ```python
    fed_model = FLModel(
        server=charlie,
        device_list=device_list,
        model=model,
        aggregator=secure_aggregator,
        strategy="fed_avg_w",
        backend="tensorflow",
    )
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

6. Lets run model

<Notebook.Cell>
  <Notebook.CodeArea prompt="[14]:" stderr={false} type="input">
    ```python
    history = fed_model.fit(
        x_train,
        y_train,
        validation_data=(x_test, y_test),
        epochs=10,
        sampler_method="batch",
        batch_size=128,
        aggregate_freq=1,
    )
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

<Notebook.Cell>
  <Notebook.CodeArea prompt="[15]:" stderr={false} type="input">
    ```python
    # Draw accuracy values for training & validation
    plt.plot(history.global_history['accuracy'])
    plt.plot(history.global_history['val_accuracy'])
    plt.title('FLModel accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Valid'], loc='upper left')
    plt.show()

    # Draw loss for training & validation
    plt.plot(history.global_history['loss'])
    plt.plot(history.global_history['val_loss'])
    plt.title('FLModel loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Valid'], loc='upper left')
    plt.show()
    ```
  </Notebook.CodeArea>

  <Notebook.FancyOutput prompt="" type="output">
    ![](../_assets/tutorial_Federate_Learning_for_Image_Classification_35_0.png)
  </Notebook.FancyOutput>

  <Notebook.FancyOutput prompt="" type="output">
    ![](../_assets/tutorial_Federate_Learning_for_Image_Classification_35_1.png)
  </Notebook.FancyOutput>
</Notebook.Cell>

<Notebook.Cell>
  <Notebook.CodeArea prompt="[16]:" stderr={false} type="input">
    ```python
    global_metric = fed_model.evaluate(x_test, y_test, batch_size=128)
    print(global_metric)
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="" stderr={false} type="output">
    <pre>
      {"([Mean(name='loss', total=748.24194, count=10000.0), Mean(name='accuracy', total=9775.0, count=10000.0)], {'alice': [Mean(name='loss', total=527.7763, count=5000.0), Mean(name='accuracy', total=4833.0, count=5000.0)], 'bob': [Mean(name='loss', total=220.46567, count=5000.0), Mean(name='accuracy', total=4942.0, count=5000.0)]})\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Contrast-experiment-to-local-training}

### Contrast experiment to local training

:target{#Model}

#### Model

The model structure is consistent with the fl model above.

:target{#Data}

#### Data

Here, we only used data after a horizontal segmentation, with a total of 20,000 samples for `Alice`.

<Notebook.Cell>
  <Notebook.CodeArea prompt="[17]:" stderr={false} type="input">
    ```python
    from tensorflow import keras
    from tensorflow.keras import layers
    from sklearn.model_selection import train_test_split


    def create_model():
        model = keras.Sequential(
            [
                keras.Input(shape=input_shape),
                layers.Conv2D(32, kernel_size=(3, 3), activation="relu"),
                layers.MaxPooling2D(pool_size=(2, 2)),
                layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
                layers.MaxPooling2D(pool_size=(2, 2)),
                layers.Flatten(),
                layers.Dropout(0.5),
                layers.Dense(num_classes, activation="softmax"),
            ]
        )
        # Compile model
        model.compile(
            loss='categorical_crossentropy', optimizer='adam', metrics=["accuracy"]
        )
        return model


    single_model = create_model()
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

<Notebook.Cell>
  <Notebook.CodeArea prompt="[18]:" stderr={false} type="input">
    ```python
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import OneHotEncoder

    alice_x = image[:10000]
    alice_y = label[:10000]
    alice_y = OneHotEncoder(sparse=False).fit_transform(alice_y.reshape(-1, 1))

    random_seed = 1234
    alice_X_train, alice_X_test, alice_y_train, alice_y_test = train_test_split(
        alice_x, alice_y, test_size=0.33, random_state=random_seed
    )
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

<Notebook.Cell>
  <Notebook.CodeArea prompt="[19]:" stderr={false} type="input">
    ```python
    single_model.fit(
        alice_X_train,
        alice_y_train,
        validation_data=(alice_X_test, alice_y_test),
        batch_size=128,
        epochs=10,
    )
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="" stderr={false} type="output">
    <pre>
      {"Epoch 1/10\n53/53 [==============================] - 1s 16ms/step - loss: 8.1856 - accuracy: 0.5194 - val_loss: 0.5021 - val_accuracy: 0.8461\nEpoch 2/10\n53/53 [==============================] - 1s 13ms/step - loss: 0.7643 - accuracy: 0.7754 - val_loss: 0.3211 - val_accuracy: 0.9024\nEpoch 3/10\n53/53 [==============================] - 1s 13ms/step - loss: 0.5189 - accuracy: 0.8452 - val_loss: 0.2557 - val_accuracy: 0.9233\nEpoch 4/10\n53/53 [==============================] - 1s 14ms/step - loss: 0.3795 - accuracy: 0.8899 - val_loss: 0.1997 - val_accuracy: 0.9388\nEpoch 5/10\n53/53 [==============================] - 1s 13ms/step - loss: 0.3246 - accuracy: 0.9024 - val_loss: 0.1864 - val_accuracy: 0.9406\nEpoch 6/10\n53/53 [==============================] - 1s 13ms/step - loss: 0.2747 - accuracy: 0.9182 - val_loss: 0.1696 - val_accuracy: 0.9464\nEpoch 7/10\n53/53 [==============================] - 1s 12ms/step - loss: 0.2245 - accuracy: 0.9324 - val_loss: 0.1484 - val_accuracy: 0.9545\nEpoch 8/10\n53/53 [==============================] - 1s 13ms/step - loss: 0.2123 - accuracy: 0.9361 - val_loss: 0.1427 - val_accuracy: 0.9570\nEpoch 9/10\n53/53 [==============================] - 1s 13ms/step - loss: 0.1884 - accuracy: 0.9425 - val_loss: 0.1290 - val_accuracy: 0.9633\nEpoch 10/10\n53/53 [==============================] - 1s 13ms/step - loss: 0.1775 - accuracy: 0.9451 - val_loss: 0.1179 - val_accuracy: 0.9627\n"}
    </pre>
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="[19]:" stderr={false} type="output">
    <pre>
      {"<keras.callbacks.History at 0x7fcd9c53f7c0>\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

The two experiments above simulated a training problem in a typical horizontal federation scenario, Alice and Bob have same type of data. Each side had only a portion of the sample, but the training objectives is the same. If Alice only uses her own data to train the model, could only obtain a model with an accuracy of 0.945. However, if Bob’s data is combined, a model with an accuracy close to 0.995 can be obtained. In addition, the generalization performance of the model jointly trained with
multi-party data will also be better.

:target{#Conclusion}

## Conclusion

- This tutorial introduces what federated learning is and how to perform horizontal federated learning in `secretFlow`.
- It can be seen from the experimental data that horizontal federation can improve the model effect by expanding the sample size and combining multi-party training.
- This tutorial uses a SecureAggregator to demonstrate, and secretflow provides a variety of aggregation schemes，for more infomation, see [Secure Aggregation](../developer/algorithm/secure_aggregation.mdx).
- next, you can use your data or model to explore how to do federate learning.
