---
git_commit: 215ee402b76decfeb01e97c0861cab9f5ec3f823
git_download_url: https://github.com/secretflow/secretflow/raw/215ee402b76decfeb01e97c0861cab9f5ec3f823/docs/tutorial/gpt2_with_puma.ipynb
git_origin_url: https://github.com/secretflow/secretflow/blob/215ee402b76decfeb01e97c0861cab9f5ec3f823/docs/tutorial/gpt2_with_puma.ipynb
git_owner: secretflow
git_repo: secretflow
git_timestamp: '2023-12-06T20:08:22+08:00'
---

:target{#GPT-2-Secure-inference-with-Puma}

# GPT-2 Secure inference with Puma

In this lab, we showcase how to run 3PC secure inference on a pre-trained [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) model for text generation with Puma.

First, we show how to use JAX and the Hugging Face Transformers library for text generation with the pre-trained GPT-2 model. After that, we show how to use Puma on the top of SPU for secure text generation with minor modifications to the plaintext counterpart.

> The following codes are demos only. It’s <strong>NOT for production</strong> due to system security concerns, please <strong>DO NOT</strong> use it directly in production.

> This tutorial may need more resources than 16c48g.

:target{#What-is-Puma?}

## What is Puma?

Puma is a fast and accurate end-to-end 3-party secure Transformer models inference framework. Puma designs high quality approximations for expensive functions, such as <InlineMath>$\mathsf{GeLU}$</InlineMath> and <InlineMath>$\mathsf{Softmax}$</InlineMath>, which significantly reduce the cost of secure inference while preserving the model performance. Additionally, we design secure <InlineMath>$\mathsf{Embedding}$</InlineMath> and <InlineMath>$\mathsf{LayerNorm}$</InlineMath> procedures that faithfully implement the desired functionality without undermining the
Transformer architecture. Puma is approximately <InlineMath>$2\times$</InlineMath> faster than the state-of-the-art MPC framework MPCFormer (ICLR 2023) and has similar accuracy as plaintext models without fine-tuning (which the previous works failed to achieve).

:target{#Text-generation-using-GPT-2-with-JAX/FLAX}

## Text generation using GPT-2 with JAX/FLAX

:target{#Install-the-transformers-library}

### Install the transformers library

<Notebook.Cell>
  <Notebook.CodeArea prompt="[ ]:" stderr={false} type="input">
    ```python
    import sys

    !{sys.executable} -m pip install transformers[flax]
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

> The JAX version required by transformers is not satisfied with SPU. But it’s ok to run with the conflicted JAX with SPU in this example.

:target{#Load-the-pre-trained-GPT-2-Model}

### Load the pre-trained GPT-2 Model

Please refer to this [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/gpt2) for more details.

<Notebook.Cell>
  <Notebook.CodeArea prompt="[2]:" stderr={false} type="input">
    ```python
    from transformers import AutoTokenizer, FlaxGPT2LMHeadModel, GPT2Config

    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    pretrained_model = FlaxGPT2LMHeadModel.from_pretrained("gpt2")
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

To hack GeLU function of GPT2, you need to change the `self.act` as `jax.nn.gelu` to hack `gelu`. For example, in `transformers/src/transformers/models/gpt2/modeling_flax_gpt2.py`, line 296:

```python
hidden_states = self.act(hidden_states)
```

is changed as

```python
hidden_states = jax.nn.gelu(hidden_states)
```

:target{#Define-the-text-generation-function}

### Define the text generation function

We use a [greedy search strategy](https://huggingface.co/blog/how-to-generate) for text generation here.

<Notebook.Cell>
  <Notebook.CodeArea prompt="[3]:" stderr={false} type="input">
    ```python
    def text_generation(input_ids, params):
        config = GPT2Config()
        model = FlaxGPT2LMHeadModel(config=config)

        for _ in range(10):
            outputs = model(input_ids=input_ids, params=params)
            next_token_logits = outputs[0][0, -1, :]
            next_token = jnp.argmax(next_token_logits)
            input_ids = jnp.concatenate([input_ids, jnp.array([[next_token]])], axis=1)
        return input_ids
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Run-text-generation-on-CPU}

### Run text generation on CPU

<Notebook.Cell>
  <Notebook.CodeArea prompt="[4]:" stderr={false} type="input">
    ```python
    import jax.numpy as jnp

    inputs_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='jax')
    outputs_ids = text_generation(inputs_ids, pretrained_model.params)

    print('-' * 65 + '\nRun on CPU:\n' + '-' * 65)
    print(tokenizer.decode(outputs_ids[0], skip_special_tokens=True))
    print('-' * 65)
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="" stderr={false} type="output">
    <pre>
      {"-----------------------------------------------------------------\nRun on CPU:\n-----------------------------------------------------------------\nI enjoy walking with my cute dog, but I'm not sure if I'll ever\n-----------------------------------------------------------------\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

Here we generate 10 tokens. Keep the generated text in mind, we are going to generate text on SPU in the next step.

:target{#Run-text-generation-on-SPU}

### Run text generation on SPU

:target{#Import-the-necessary-libraries-and-config-the-optimizations}

#### Import the necessary libraries and config the optimizations

<Notebook.Cell>
  <Notebook.CodeArea prompt="[ ]:" stderr={false} type="input">
    ```python
    import secretflow as sf
    from typing import Any, Callable, Dict, Optional, Tuple, Union
    import jax.nn as jnn
    import flax.linen as nn
    from flax.linen.linear import Array
    import jax
    import argparse
    import spu.utils.distributed as ppd
    import spu.intrinsic as intrinsic
    import spu.spu_pb2 as spu_pb2
    from contextlib import contextmanager

    copts = spu_pb2.CompilerOptions()
    copts.enable_pretty_print = False
    copts.xla_pp_kind = 2
    # enable x / broadcast(y) -> x * broadcast(1/y)
    copts.enable_optimize_denominator_with_broadcast = True
    Array = Any

    # In case you have a running secretflow runtime already.
    sf.shutdown()
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{id="Define-the-Softmax-hijack-function."}

#### Define the Softmax hijack function.

<Notebook.Cell>
  <Notebook.CodeArea prompt="[ ]:" stderr={false} type="input">
    ```python
    def hack_softmax(
        x: Array,
        axis: Optional[Union[int, Tuple[int, ...]]] = -1,
        where: Optional[Array] = None,
        initial: Optional[Array] = None,
    ) -> Array:
        x_max = jnp.max(x, axis, where=where, initial=initial, keepdims=True)
        x = x - x_max

        # exp on large negative is clipped to zero
        b = x > -14
        nexp = jnp.exp(x)

        divisor = jnp.sum(nexp, axis, where=where, keepdims=True)

        return b * (nexp / divisor)


    @contextmanager
    def hack_softmax_context(msg: str, enabled: bool = False):
        if not enabled:
            yield
            return
        # hijack some target functions
        raw_softmax = jnn.softmax
        jnn.softmax = hack_softmax
        yield
        # recover back
        jnn.softmax = raw_softmax
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Define-the-GeLU-hijack-function}

#### Define the GeLU hijack function

<Notebook.Cell>
  <Notebook.CodeArea prompt="[ ]:" stderr={false} type="input">
    ```python
    def hack_gelu(
        x: Array,
        axis: Optional[Union[int, Tuple[int, ...]]] = -1,
        where: Optional[Array] = None,
        initial: Optional[Array] = None,
    ) -> Array:
        b0 = x < -4.0
        b1 = x < -1.95
        b2 = x > 3.0
        b3 = b1 ^ b2 ^ True  # x in [-1.95, 3.0]
        b4 = b0 ^ b1  # x in [-4, -1.95]

        # seg1 = a[3] * x^3 + a[2] * x^2 + a[1] * x + a[0]
        # seg2 = b[6] * x^6 + b[4] * x^4 + b[2] * x^2 + b[1] * x + b[0]
        a_coeffs = jnp.array(
            [
                -0.5054031199708174,
                -0.42226581151983866,
                -0.11807612951181953,
                -0.011034134030615728,
            ]
        )
        b_coeffs = jnp.array(
            [
                0.008526321541038084,
                0.5,
                0.3603292692789629,
                0.0,
                -0.037688200365904236,
                0.0,
                0.0018067462606141187,
            ]
        )
        x2 = jnp.square(x)
        x3 = jnp.multiply(x, x2)
        x4 = jnp.square(x2)
        x6 = jnp.square(x3)

        seg1 = a_coeffs[3] * x3 + a_coeffs[2] * x2 + a_coeffs[1] * x + a_coeffs[0]
        seg2 = (
            b_coeffs[6] * x6
            + b_coeffs[4] * x4
            + b_coeffs[2] * x2
            + b_coeffs[1] * x
            + b_coeffs[0]
        )

        ret = b2 * x + b4 * seg1 + b3 * seg2

        return ret


    @contextmanager
    def hack_gelu_context(msg: str, enabled: bool = False):
        if not enabled:
            yield
            return
        # hijack some target functions
        raw_gelu = jnn.gelu
        jnn.gelu = hack_gelu
        yield
        # recover back
        jnn.gelu = raw_gelu
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Launch-Puma-on-GPT2:}

#### Launch Puma on GPT2:

<Notebook.Cell>
  <Notebook.CodeArea prompt="[5]:" stderr={false} type="input">
    ```python
    sf.init(['alice', 'bob', 'carol'], address='local')

    alice, bob = sf.PYU('alice'), sf.PYU('bob')
    conf = sf.utils.testing.cluster_def(['alice', 'bob', 'carol'])
    conf['runtime_config']['protocol'] = 'ABY3'
    conf['runtime_config']['field'] = 'FM64'
    conf['runtime_config']['fxp_exp_mode'] = 0
    conf['runtime_config']['fxp_exp_iters'] = 5

    spu = sf.SPU(conf)


    def get_model_params():
        pretrained_model = FlaxGPT2LMHeadModel.from_pretrained("gpt2")
        return pretrained_model.params


    def get_token_ids():
        tokenizer = AutoTokenizer.from_pretrained("gpt2")
        return tokenizer.encode('I enjoy walking with my cute dog', return_tensors='jax')


    model_params = alice(get_model_params)()
    input_token_ids = bob(get_token_ids)()

    device = spu
    model_params_, input_token_ids_ = model_params.to(device), input_token_ids.to(device)

    with hack_softmax_context("hijack jax softmax", enabled=True), hack_gelu_context(
        "hack jax gelu", enabled=True
    ):
        output_token_ids = spu(text_generation, copts=copts)(
            input_token_ids_, model_params_
        )
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="" stderr={false} type="output">
    <pre>
      {"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"}
    </pre>
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="" stderr={false} type="output">
    <pre>
      {"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n        - Avoid using `tokenizers` before the fork if possible\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"}
    </pre>
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="" stderr={false} type="output">
    <pre>
      <span className="ansi-cyan-fg">{"(_run pid=2109408)"}</span>{" [2023-06-15 17:08:24.221] [info] [thread_pool.cc:30] Create a fixed thread pool with size 127\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Check-the-Puma-output}

### Check the Puma output

As you can see, it’s very easy to run GPT-2 inference on Puma. Now let’s reveal the generated text from Puma.

<Notebook.Cell>
  <Notebook.CodeArea prompt="[6]:" stderr={false} type="input">
    ```python
    outputs_ids = sf.reveal(output_token_ids)
    print('-' * 65 + '\nRun on SPU:\n' + '-' * 65)
    print(tokenizer.decode(outputs_ids[0], skip_special_tokens=True))
    print('-' * 65)
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="" stderr={false} type="output">
    <pre>
      <span className="ansi-cyan-fg">{"(SPURuntime(device_id=None, party=bob) pid=2121303)"}</span>{" 2023-06-15 17:09:32.011 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n"}<span className="ansi-cyan-fg">{"(SPURuntime(device_id=None, party=alice) pid=2121301)"}</span>{" 2023-06-15 17:09:32.011 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n"}<span className="ansi-cyan-fg">{"(SPURuntime(device_id=None, party=carol) pid=2121304)"}</span>{" 2023-06-15 17:09:32.011 [info] [thread_pool.cc:ThreadPool:30] Create a fixed thread pool with size 127\n-----------------------------------------------------------------\nRun on SPU:\n-----------------------------------------------------------------\nI enjoy walking with my cute dog, but I'm not sure if I'll ever\n-----------------------------------------------------------------\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

As we can see, the generated text from Puma is exactly same as the generated text from CPU!

This is the end of the lab. For more benchmarks about Puma, please refer to: [https://github.com/secretflow/spu/tree/main/examples/python/ml/flax\_llama7b](https://github.com/secretflow/spu/tree/main/examples/python/ml/flax_llama7b)
