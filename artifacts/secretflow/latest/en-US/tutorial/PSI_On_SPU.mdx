:target{#PSI-On-SPU}

# PSI On SPU

> The following codes are demos only. It’s <strong>NOT for production</strong> due to system security concerns, please <strong>DO NOT</strong> use it directly in production.

PSI([Private Set Intersection](https://en.wikipedia.org/wiki/Private_set_intersection)) is a cryptographic technique that allows two parties holding sets to compare encrypted versions of these sets in order to compute the intersection. In this scenario, neither party reveals anything to the counterparty except for the elements in the intersection.

In SecretFlow, SPU device supports three PSI protocol:

- [ECDH](https://ieeexplore.ieee.org/document/6234849/)：semi-honest, based on public key encryption, suitable for small datasets.
- [KKRT](https://eprint.iacr.org/2016/799.pdf)：semi-host, based on cuckoo hashing and OT extension, suitable for large datasets.
- [BC22PCG](https://eprint.iacr.org/2022/334): semi-host, psi from pseudorandom correlation generators.

Before we start, we need to initialize the environment. The following three nodes `alice`, `bob`, and `carol` are created on a single machine to simulate multiple participants.

<Notebook.Cell>
  <Notebook.CodeArea prompt="[1]:" stderr={false} type="input">
    ```python
    import secretflow as sf

    # Check the version of your SecretFlow
    print('The version of SecretFlow: {}'.format(sf.__version__))

    # In case you have a running secretflow runtime already.
    sf.shutdown()

    sf.init(['alice', 'bob', 'carol'], address='local')
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Preparing-dataset}

## Preparing dataset

First, we need a dataset for constructing vertical partitioned scenarios. For simplicity, we use [iris](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html) dataset here. We add two columns to it for subsequent single-column and multi-column intersection demonstrations

- uid：Sample unique ID.
- month：Simulate a scenario where samples are generated monthly. The first 50% of the samples are generated in January, and the last 50% of the samples are generated in February.

<Notebook.Cell>
  <Notebook.CodeArea prompt="[2]:" stderr={false} type="input">
    ```python
    import numpy as np
    from sklearn.datasets import load_iris

    data, target = load_iris(return_X_y=True, as_frame=True)
    data['uid'] = np.arange(len(data)).astype('str')
    data['month'] = ['Jan'] * 75 + ['Feb'] * 75

    data
    ```
  </Notebook.CodeArea>

  <Notebook.FancyOutput prompt="[2]:" type="output">
    <div>
      <style scoped={true}>
        {"\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n"}
      </style>

      <table border={1} className="dataframe">
        <thead>
          <tr style={{"textAlign":"right"}}>
            <th /><th>{"sepal length (cm)"}</th><th>{"sepal width (cm)"}</th><th>{"petal length (cm)"}</th><th>{"petal width (cm)"}</th><th>{"uid"}</th><th>{"month"}</th>
          </tr>
        </thead>

        <tbody>
          <tr>
            <th>{"0"}</th><td>{"5.1"}</td><td>{"3.5"}</td><td>{"1.4"}</td><td>{"0.2"}</td><td>{"0"}</td><td>{"Jan"}</td>
          </tr>

          <tr>
            <th>{"1"}</th><td>{"4.9"}</td><td>{"3.0"}</td><td>{"1.4"}</td><td>{"0.2"}</td><td>{"1"}</td><td>{"Jan"}</td>
          </tr>

          <tr>
            <th>{"2"}</th><td>{"4.7"}</td><td>{"3.2"}</td><td>{"1.3"}</td><td>{"0.2"}</td><td>{"2"}</td><td>{"Jan"}</td>
          </tr>

          <tr>
            <th>{"3"}</th><td>{"4.6"}</td><td>{"3.1"}</td><td>{"1.5"}</td><td>{"0.2"}</td><td>{"3"}</td><td>{"Jan"}</td>
          </tr>

          <tr>
            <th>{"4"}</th><td>{"5.0"}</td><td>{"3.6"}</td><td>{"1.4"}</td><td>{"0.2"}</td><td>{"4"}</td><td>{"Jan"}</td>
          </tr>

          <tr>
            <th>{"..."}</th><td>{"..."}</td><td>{"..."}</td><td>{"..."}</td><td>{"..."}</td><td>{"..."}</td><td>{"..."}</td>
          </tr>

          <tr>
            <th>{"145"}</th><td>{"6.7"}</td><td>{"3.0"}</td><td>{"5.2"}</td><td>{"2.3"}</td><td>{"145"}</td><td>{"Feb"}</td>
          </tr>

          <tr>
            <th>{"146"}</th><td>{"6.3"}</td><td>{"2.5"}</td><td>{"5.0"}</td><td>{"1.9"}</td><td>{"146"}</td><td>{"Feb"}</td>
          </tr>

          <tr>
            <th>{"147"}</th><td>{"6.5"}</td><td>{"3.0"}</td><td>{"5.2"}</td><td>{"2.0"}</td><td>{"147"}</td><td>{"Feb"}</td>
          </tr>

          <tr>
            <th>{"148"}</th><td>{"6.2"}</td><td>{"3.4"}</td><td>{"5.4"}</td><td>{"2.3"}</td><td>{"148"}</td><td>{"Feb"}</td>
          </tr>

          <tr>
            <th>{"149"}</th><td>{"5.9"}</td><td>{"3.0"}</td><td>{"5.1"}</td><td>{"1.8"}</td><td>{"149"}</td><td>{"Feb"}</td>
          </tr>
        </tbody>
      </table>

      <p>{"150 rows × 6 columns"}</p>
    </div>
  </Notebook.FancyOutput>
</Notebook.Cell>

In the actual scenario, the sample data is provided by each participant, and the fields for intersection need to be agreed in advance:

- The intersection field can be single or multiple.
- The intersection field must be unique. If there is a duplicate, it needs to be deduplicated in advance.

For example, The following is the data provided by alice for PSI intersection, the intersection field is `uid` and `month`，we can see that \[1, ‘Jan’] is duplicated.

```none
alice.csv
---------
uid   month   c0
1     Jan     5.8
2     Jan     5.4
1     Jan     5.8
1     Feb     7.4
```

The data after deduplication is

```none
alice.csv
---------
uid   month   c0
1     Jan     5.8
2     Jan     5.4
1     Feb     7.4
```

We randomly sample the iris data three times to simulate the data provided by `alice`, `bob`, and `carol`, and the three data are in an unaligned state.

<Notebook.Cell>
  <Notebook.CodeArea prompt="[3]:" stderr={false} type="input">
    ```python
    import os

    os.makedirs('.data', exist_ok=True)
    da, db, dc = data.sample(frac=0.9), data.sample(frac=0.8), data.sample(frac=0.7)

    da.to_csv('.data/alice.csv', index=False)
    db.to_csv('.data/bob.csv', index=False)
    dc.to_csv('.data/carol.csv', index=False)
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Two-parties-PSI}

## Two parties PSI

We virtualize three logical devices on the physical device:

- alice, bob: PYU device, responsible for the local plaintext computation of the participant.
- spu：SPUdevice, consists of alice and bob, responsible for the ciphertext calculation of the two parties.

<Notebook.Cell>
  <Notebook.CodeArea prompt="[4]:" stderr={false} type="input">
    ```python
    alice, bob = sf.PYU('alice'), sf.PYU('bob')
    spu = sf.SPU(sf.utils.testing.cluster_def(['alice', 'bob']))
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Single-column-PSI}

### Single-column PSI

Next, we use `uid` to intersect the two data, SPU provide `psi_csv` which take the csv file as input and generate the csv file after the intersection. The default protocol is KKRT.

<Notebook.Cell>
  <Notebook.CodeArea prompt="[5]:" stderr={false} type="input">
    ```python
    input_path = {alice: '.data/alice.csv', bob: '.data/bob.csv'}
    output_path = {alice: '.data/alice_psi.csv', bob: '.data/bob_psi.csv'}
    spu.psi_csv('uid', input_path, output_path, 'alice')
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

To check the correctness of the results, we use [pandas.DataFrame.join](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html) to inner join da and db. It can be seen that the two data have been aligned according to `uid` and sorted according to their lexicographical order.

<Notebook.Cell>
  <Notebook.CodeArea prompt="[6]:" stderr={false} type="input">
    ```python
    import pandas as pd

    df = da.join(db.set_index('uid'), on='uid', how='inner', rsuffix='_bob', sort=True)
    expected = df[da.columns].astype({'uid': 'int64'}).reset_index(drop=True)

    da_psi = pd.read_csv('.data/alice_psi.csv')
    db_psi = pd.read_csv('.data/bob_psi.csv')

    pd.testing.assert_frame_equal(da_psi, expected)
    pd.testing.assert_frame_equal(db_psi, expected)

    da_psi
    ```
  </Notebook.CodeArea>

  <Notebook.FancyOutput prompt="[6]:" type="output">
    <div>
      <style scoped={true}>
        {"\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n"}
      </style>

      <table border={1} className="dataframe">
        <thead>
          <tr style={{"textAlign":"right"}}>
            <th /><th>{"sepal length (cm)"}</th><th>{"sepal width (cm)"}</th><th>{"petal length (cm)"}</th><th>{"petal width (cm)"}</th><th>{"uid"}</th><th>{"month"}</th>
          </tr>
        </thead>

        <tbody>
          <tr>
            <th>{"0"}</th><td>{"6.3"}</td><td>{"3.3"}</td><td>{"6.0"}</td><td>{"2.5"}</td><td>{"100"}</td><td>{"Feb"}</td>
          </tr>

          <tr>
            <th>{"1"}</th><td>{"5.8"}</td><td>{"2.7"}</td><td>{"5.1"}</td><td>{"1.9"}</td><td>{"101"}</td><td>{"Feb"}</td>
          </tr>

          <tr>
            <th>{"2"}</th><td>{"7.1"}</td><td>{"3.0"}</td><td>{"5.9"}</td><td>{"2.1"}</td><td>{"102"}</td><td>{"Feb"}</td>
          </tr>

          <tr>
            <th>{"3"}</th><td>{"6.3"}</td><td>{"2.9"}</td><td>{"5.6"}</td><td>{"1.8"}</td><td>{"103"}</td><td>{"Feb"}</td>
          </tr>

          <tr>
            <th>{"4"}</th><td>{"6.5"}</td><td>{"3.0"}</td><td>{"5.8"}</td><td>{"2.2"}</td><td>{"104"}</td><td>{"Feb"}</td>
          </tr>

          <tr>
            <th>{"..."}</th><td>{"..."}</td><td>{"..."}</td><td>{"..."}</td><td>{"..."}</td><td>{"..."}</td><td>{"..."}</td>
          </tr>

          <tr>
            <th>{"101"}</th><td>{"5.6"}</td><td>{"2.7"}</td><td>{"4.2"}</td><td>{"1.3"}</td><td>{"94"}</td><td>{"Feb"}</td>
          </tr>

          <tr>
            <th>{"102"}</th><td>{"5.7"}</td><td>{"2.9"}</td><td>{"4.2"}</td><td>{"1.3"}</td><td>{"96"}</td><td>{"Feb"}</td>
          </tr>

          <tr>
            <th>{"103"}</th><td>{"6.2"}</td><td>{"2.9"}</td><td>{"4.3"}</td><td>{"1.3"}</td><td>{"97"}</td><td>{"Feb"}</td>
          </tr>

          <tr>
            <th>{"104"}</th><td>{"5.1"}</td><td>{"2.5"}</td><td>{"3.0"}</td><td>{"1.1"}</td><td>{"98"}</td><td>{"Feb"}</td>
          </tr>

          <tr>
            <th>{"105"}</th><td>{"5.7"}</td><td>{"2.8"}</td><td>{"4.1"}</td><td>{"1.3"}</td><td>{"99"}</td><td>{"Feb"}</td>
          </tr>
        </tbody>
      </table>

      <p>{"106 rows × 6 columns"}</p>
    </div>
  </Notebook.FancyOutput>
</Notebook.Cell>

:target{#Multi-columns-PSI}

### Multi-columns PSI

We can also use multiple fields to intersect, the following demonstrates the use of `uid` and `month` to intersect two data. In terms of implementation, multiple fields are concatenated into a string, so please ensure that there is no duplication of the multi-column composite primary key.

<Notebook.Cell>
  <Notebook.CodeArea prompt="[7]:" stderr={false} type="input">
    ```python
    spu.psi_csv(['uid', 'month'], input_path, output_path, 'alice')
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

Similarly, we use pandas.DataFrame.join to verify the correctness of the result, we can see that the two data have been aligned according to `uid` and `month`, and sorted according to their lexicographical order.

<Notebook.Cell>
  <Notebook.CodeArea prompt="[8]:" stderr={false} type="input">
    ```python
    df = da.join(
        db.set_index(['uid', 'month']),
        on=['uid', 'month'],
        how='inner',
        rsuffix='_bob',
        sort=True,
    )
    expected = df[da.columns].astype({'uid': 'int64'}).reset_index(drop=True)

    da_psi = pd.read_csv('.data/alice_psi.csv')
    db_psi = pd.read_csv('.data/bob_psi.csv')

    pd.testing.assert_frame_equal(da_psi, expected)
    pd.testing.assert_frame_equal(db_psi, expected)
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#Three-parties-PSI}

## Three parties PSI

Next, we add a third-party `carol`, and create a PYU device for it, as well as an SPU device built by the third party.

<Notebook.Cell>
  <Notebook.CodeArea prompt="[9]:" stderr={false} type="input">
    ```python
    carol = sf.PYU('carol')
    spu_3pc = sf.SPU(sf.utils.testing.cluster_def(['alice', 'bob', 'carol']))
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

Then, use `uid` and `month` as the composite primary key to perform a three-way negotiation. It should be noted that the three-way negotiation only supports the ECDH protocol for the time being.

<Notebook.Cell>
  <Notebook.CodeArea prompt="[10]:" stderr={false} type="input">
    ```python
    input_path = {alice: '.data/alice.csv', bob: '.data/bob.csv', carol: '.data/carol.csv'}
    output_path = {
        alice: '.data/alice_psi.csv',
        bob: '.data/bob_psi.csv',
        carol: '.data/carol_psi.csv',
    }
    spu_3pc.psi_csv(
        ['uid', 'month'], input_path, output_path, 'alice', protocol='ECDH_PSI_3PC'
    )
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

Similarly, we use pandas.DataFrame.join to verify the correctness of the result.

<Notebook.Cell>
  <Notebook.CodeArea prompt="[11]:" stderr={false} type="input">
    ```python
    keys = ['uid', 'month']
    df = da.join(db.set_index(keys), on=keys, how='inner', rsuffix='_bob', sort=True).join(
        dc.set_index(keys), on=keys, how='inner', rsuffix='_carol', sort=True
    )
    expected = df[da.columns].astype({'uid': 'int64'}).reset_index(drop=True)

    da_psi = pd.read_csv('.data/alice_psi.csv')
    db_psi = pd.read_csv('.data/bob_psi.csv')
    dc_psi = pd.read_csv('.data/carol_psi.csv')

    pd.testing.assert_frame_equal(da_psi, expected)
    pd.testing.assert_frame_equal(db_psi, expected)
    pd.testing.assert_frame_equal(dc_psi, expected)
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#What’s-Next}

## What’s Next

OK! Through this tutorial, we have seen how to do two-party and three-party data intersections via SPU. After completing the data intersection, we can perform machine learning modeling on the aligned dataset.

- [Logistic Regression On SPU](lr_with_spu.mdx): Logistic regression modeling on SPU using JAX.
- [Neural Network on SPU](nn_with_spu.mdx): Neural Network Modeling on SPU with JAX.
- [Basic Split Learning](split_learning_gnn.mdx): Neural Network Modeling with TensorFlow and Split Learning.
