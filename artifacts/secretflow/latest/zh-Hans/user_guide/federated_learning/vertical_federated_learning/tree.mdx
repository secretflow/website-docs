---
git_download_url: https://github.com/secretflow/secretflow/raw/f2c46f98224a42f29bca20751a569bbfac39f750/docs/user_guide/federated_learning/vertical_federated_learning/tree.md
git_last_modified_commit: 77f923e99e01e9757cc7ebaeb92e80aec6eb4711
git_last_modified_time: '2023-07-31T16:54:37+08:00'
git_origin_url: https://github.com/secretflow/secretflow/blob/f2c46f98224a42f29bca20751a569bbfac39f750/docs/user_guide/federated_learning/vertical_federated_learning/tree.md
git_owner: secretflow
git_repo: secretflow
git_revision_commit: f2c46f98224a42f29bca20751a569bbfac39f750
git_revision_time: '2024-01-03T19:41:12+08:00'
---

:target{#vertically-federated-xgb-secureboost}

# 垂直联邦XGB (SecureBoost)

在垂直联邦学习场景中，数据根据特征进行垂直分割，这意味着每个参与者的数据样本是一致的，但具有不同的列和类型。

:target{#introduction-to-secureboost}

## SecureBoost简介

原始论文: [SecureBoost](https://arxiv.org/abs/1901.08755)

在垂直联邦学习场景中，每个参与方都拥有具有相同样本但不同特征空间的数据。SecureBoost优先保护标签持有者的信息，并旨在与原始的XGBoost算法一样准确。

与其MPC技术驱动的对应物——基于密钥共享的XGB相比，SecureBoost通常更快。更具体地说，SecureBoost的计算成本比ss\_xgb高，但后者通常受到网络带宽的限制。换句话说，当我们拥有更多的CPU计算资源但较少的网络资源时，SecureBoost更快。

得到了HEU设备的支持, 我们的SecureBoost实现提供了高性能和尖端速度。

自secretflow 1.0版本开始，我们进一步优化了性能，并支持了一些高级训练功能，包括lightGBM风格（叶子优先和GOSS）训练，仅使用标签持有者的数据训练第一棵树等。

:target{#tutorial}

## 教程

请查看这个简单的[教程](../../../tutorial/SecureBoost.mdx)

:target{#security-warning}

## 安全提示

请注意，联邦树模型算法 [SecureBoost](https://arxiv.org/abs/1901.08755) 不是可证明安全的算法。存在可能导致数据泄露的已知[攻击](https://arxiv.org/pdf/2011.09290.pdf) 。因此，当担心数据安全时，我们建议使用 [MPC-XGB](https://arxiv.org/abs/2005.08479) 而不是 SecureBoost。查看我们的[MPC-XGB实现](../../mpc_ml/decision_tree.mdx)。
