---
git_download_url: https://github.com/secretflow/secretflow/raw/215ee402b76decfeb01e97c0861cab9f5ec3f823/docs/tutorial/CustomDataLoaderTF.ipynb
git_last_modified_commit: a06f2ac1994afc5af895824e2000a848dd1e4673
git_last_modified_time: '2023-08-22T13:04:56+08:00'
git_origin_url: https://github.com/secretflow/secretflow/blob/215ee402b76decfeb01e97c0861cab9f5ec3f823/docs/tutorial/CustomDataLoaderTF.ipynb
git_owner: secretflow
git_repo: secretflow
git_revision_commit: 215ee402b76decfeb01e97c0861cab9f5ec3f823
git_revision_time: '2023-12-06T20:08:22+08:00'
---

:target{#在-SecretFlow-中使用自定义-DataBuilder（TensorFlow）}

# 在 SecretFlow 中使用自定义 DataBuilder（TensorFlow）

以下代码仅作为示例，请勿在生产环境直接使用。

本教程将展示下，怎样在 SecretFlow 的多方安全环境中，如何使用自定义 DataBuilder 模式加载数据，并训练模型。本教程将使用 Flower 数据集的图像分类任务来进行介绍，如何使用自定义 DataBuilder 完成联邦学习。

:target{#环境设置}

## 环境设置

<Notebook.Cell>
  <Notebook.CodeArea prompt="[1]:" stderr={false} type="input">
    ```python
    %load_ext autoreload
    %autoreload 2
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

<Notebook.Cell>
  <Notebook.CodeArea prompt="[2]:" stderr={false} type="input">
    ```python
    import secretflow as sf

    # Check the version of your SecretFlow
    print('The version of SecretFlow: {}'.format(sf.__version__))

    # In case you have a running secretflow runtime already.
    sf.shutdown()
    sf.init(['alice', 'bob', 'charlie'], address="local", log_to_driver=False)
    alice, bob, charlie = sf.PYU('alice'), sf.PYU('bob'), sf.PYU('charlie')
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{#接口介绍}

## 接口介绍

我们在 SecretFlow 的 `FLModel` 中支持了自定义 DataBuilder 的读取方式，可以方便用户根据需求更灵活的处理数据输入。下面我们以一个例子来展示下，如何使用自定义 DataBuilder 来进行联邦模型训练。

使用 DataBuilder 的步骤：

1. 使用单机版本引擎（tensorflow，pytorch）进行开发，得到 Dataset 的 Builder 函数。
2. 将各方的 Builder 函数进行 wrap，得到 `create_dataset_builder` 。<em>注：dataset\_builder 函数需要传入 stage 参数</em>
3. 构造 data\_builder\_dict \[PYU, dataset\_builder]。
4. 将得到的 data\_builder\_dict 传入 `fit` 函数的 `dataset_builder`。同时 x 参数位置传入 dataset\_builder 中需要的输入。（比如：本例中传入的输入是实际使用的图像路径）。

在 FLModel 中使用 DataBuilder 需要预先定义 `data_builder_dict`。需要能够返回 `tf.dataset` 和 `steps_per_epoch`。而且各方返回的 steps\_per\_epoch 必须保持一致。

```python
data_builder_dict =
        {
            alice: create_alice_dataset_builder(
                batch_size=32,
            ), # create_alice_dataset_builder must return (Dataset, steps_per_epoch)
            bob: create_bob_dataset_builder(
                batch_size=32,
            ), # create_bob_dataset_builder must return (Dataset, steps_per_epochstep_per_epochs)
        }
```

:target{#下载数据}

## 下载数据

Flower 数据集介绍：flower 数据集是一个包含了 5 种花卉（雏菊、蒲公英、玫瑰、向日葵、郁金香）共计 4323 张彩色图片的数据集。每种花卉都有多个角度和不同光照下的图片，每张图片的分辨率为 320x240。这个数据集常用于图像分类和机器学习算法的训练与测试。数据集中每个类别的数量分别是：daisy（633），dandelion（898），rose（641），sunflower（699），tulip（852）

下载地址: [http://download.tensorflow.org/example\_images/flower\_photos.tgz](http://download.tensorflow.org/example_images/flower_photos.tgz)

![flower\_dataset\_demo.png](../_assets/flower_dataset_demo.png)

:target{#下载数据并解压}

### 下载数据并解压

<Notebook.Cell>
  <Notebook.CodeArea prompt="[3]:" stderr={false} type="input">
    ```python
    import tempfile
    import tensorflow as tf

    _temp_dir = tempfile.mkdtemp()
    path_to_flower_dataset = tf.keras.utils.get_file(
        "flower_photos",
        "https://secretflow-data.oss-accelerate.aliyuncs.com/datasets/tf_flowers/flower_photos.tgz",
        untar=True,
        cache_dir=_temp_dir,
    )
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="" stderr={false} type="output">
    <pre>
      {"Downloading data from https://secretflow-data.oss-accelerate.aliyuncs.com/datasets/tf_flowers/flower_photos.tgz\n67588319/67588319 [==============================] - 1s 0us/step\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

接下来我们开始构造自定义 DataBuilder

:target{id="1.-使用单机引擎开发-DataBuilder"}

## 1. 使用单机引擎开发 DataBuilder

我们在开发 DataBuilder 的时候可以自由的按照单机开发的逻辑即可。目的是构建一个 `tf.dataset` 对象即可。

<Notebook.Cell>
  <Notebook.CodeArea prompt="[4]:" stderr={false} type="input">
    ```python
    import math
    import tensorflow as tf

    img_height = 180
    img_width = 180
    batch_size = 32
    # In this example, we use the TensorFlow interface for development.
    data_set = tf.keras.utils.image_dataset_from_directory(
        path_to_flower_dataset,
        validation_split=0.2,
        subset="both",
        seed=123,
        image_size=(img_height, img_width),
        batch_size=batch_size,
    )
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="" stderr={false} type="output">
    <pre>
      {"Found 436 files belonging to 5 classes.\nUsing 349 files for training.\nUsing 87 files for validation.\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

<Notebook.Cell>
  <Notebook.CodeArea prompt="[5]:" stderr={false} type="input">
    ```python
    train_set = data_set[0]
    test_set = data_set[1]
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

<Notebook.Cell>
  <Notebook.CodeArea prompt="[6]:" stderr={false} type="input">
    ```python
    print(type(train_set), type(test_set))
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="" stderr={false} type="output">
    <pre>
      {"<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'> <class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

<Notebook.Cell>
  <Notebook.CodeArea prompt="[7]:" stderr={false} type="input">
    ```python
    x, y = next(iter(train_set))
    print(f"x.shape = {x.shape}")
    print(f"y.shape = {y.shape}")
    ```
  </Notebook.CodeArea>

  <Notebook.CodeArea prompt="" stderr={false} type="output">
    <pre>
      {"x.shape = (32, 180, 180, 3)\ny.shape = (32,)\n"}
    </pre>
  </Notebook.CodeArea>
</Notebook.Cell>

:target{id="2.-将开发完成的-DataBuilder-进行包装（wrap）"}

## 2. 将开发完成的 DataBuilder 进行包装（wrap）

我们开发好的 DataBuilder 在运行是需要分发到各个执行机器上去执行，为了序列化，我们需要把他们进行 wrap。需要注意的是： <strong>FLModel 要求传入的DataBuilder需要返回两个结果（data\_set，steps\_per\_epoch）。</strong>

<Notebook.Cell>
  <Notebook.CodeArea prompt="[8]:" stderr={false} type="input">
    ```python
    def create_dataset_builder(
        batch_size=32,
    ):
        def dataset_builder(folder_path, stage="train"):
            import math

            import tensorflow as tf

            img_height = 180
            img_width = 180
            data_set = tf.keras.utils.image_dataset_from_directory(
                folder_path,
                validation_split=0.2,
                subset="both",
                seed=123,
                image_size=(img_height, img_width),
                batch_size=batch_size,
            )
            if stage == "train":
                train_dataset = data_set[0]
                train_step_per_epoch = math.ceil(len(data_set[0].file_paths) / batch_size)
                return train_dataset, train_step_per_epoch
            elif stage == "eval":
                eval_dataset = data_set[1]
                eval_step_per_epoch = math.ceil(len(data_set[1].file_paths) / batch_size)
                return eval_dataset, eval_step_per_epoch

        return dataset_builder
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{id="3.-构建-dataset_builder_dict"}

## 3. 构建 dataset\_builder\_dict

在水平场景，我们各方处理数据的逻辑是一样的，所以只需要一个 wrap 后的DataBuilder构造方法即可。接下来我们构建 `dataset_builder_dict`

<Notebook.Cell>
  <Notebook.CodeArea prompt="[9]:" stderr={false} type="input">
    ```python
    data_builder_dict = {
        alice: create_dataset_builder(
            batch_size=32,
        ),
        bob: create_dataset_builder(
            batch_size=32,
        ),
    }
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

:target{id="4.-得到-dataset_builder_dict-后我们就可以传入模型进行使用了"}

## 4. 得到 dataset\_builder\_dict 后我们就可以传入模型进行使用了

接下来我们定义模型，并使用上面构造好的自定义数据进行训练

<Notebook.Cell>
  <Notebook.CodeArea prompt="[10]:" stderr={false} type="input">
    ```python
    def create_conv_flower_model(input_shape, num_classes, name='model'):
        def create_model():
            from tensorflow import keras

            # Create model

            model = keras.Sequential(
                [
                    keras.Input(shape=input_shape),
                    tf.keras.layers.Rescaling(1.0 / 255),
                    tf.keras.layers.Conv2D(32, 3, activation='relu'),
                    tf.keras.layers.MaxPooling2D(),
                    tf.keras.layers.Conv2D(32, 3, activation='relu'),
                    tf.keras.layers.MaxPooling2D(),
                    tf.keras.layers.Conv2D(32, 3, activation='relu'),
                    tf.keras.layers.MaxPooling2D(),
                    tf.keras.layers.Flatten(),
                    tf.keras.layers.Dense(128, activation='relu'),
                    tf.keras.layers.Dense(num_classes),
                ]
            )
            # Compile model
            model.compile(
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                optimizer='adam',
                metrics=["accuracy"],
            )
            return model

        return create_model
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

<Notebook.Cell>
  <Notebook.CodeArea prompt="[11]:" stderr={false} type="input">
    ```python
    from secretflow.ml.nn import FLModel
    from secretflow.security.aggregation import SecureAggregator
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

<Notebook.Cell>
  <Notebook.CodeArea prompt="[12]:" stderr={false} type="input">
    ```python
    device_list = [alice, bob]
    aggregator = SecureAggregator(charlie, [alice, bob])

    # prepare model
    num_classes = 5
    input_shape = (180, 180, 3)

    # keras model
    model = create_conv_flower_model(input_shape, num_classes)


    fed_model = FLModel(
        device_list=device_list,
        model=model,
        aggregator=aggregator,
        backend="tensorflow",
        strategy="fed_avg_w",
        random_seed=1234,
    )
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

我们构造好的 dataset builder 的输入是图像数据集的路径，所以这里需要将输入的数据设置为一个 `Dict`。

```python
data = {
    alice: folder_path_of_alice,
    bob: folder_path_of_bob
}
```

<Notebook.Cell>
  <Notebook.CodeArea prompt="[13]:" stderr={false} type="input">
    ```python
    data = {
        alice: path_to_flower_dataset,
        bob: path_to_flower_dataset,
    }
    history = fed_model.fit(
        data,
        None,
        validation_data=data,
        epochs=5,
        batch_size=32,
        aggregate_freq=2,
        sampler_method="batch",
        random_seed=1234,
        dp_spent_step_freq=1,
        dataset_builder=data_builder_dict,
    )
    ```
  </Notebook.CodeArea>
</Notebook.Cell>

接下来，您可以使用自己的数据集来进行尝试
